use uuid::Uuid;
use chrono::Utc;
use tracing::{debug, info};
use std::{future::Future, pin::Pin, sync::Arc, time::Duration};
use crate::{utils::config::Configuration, utils::errors::InternalError};
use mongodb::{Collection, Database, bson::{Document, doc}, options::FindOneOptions};
use tokio_retry::Retry;
use tokio_retry::strategy::{ExponentialBackoff, jitter};

// WORK IN PROGRESS
// The db-Lock requires tokio_retry which requires tokio 1.x so we must wait for actix 4...


///
/// Represents a list of run-once MongoDB operations to transform the database schema.
/// These are run in sequence of declaration - which should only ever be APPENDED to!
///
/// This mean you have to think of this array of updates as an event source. If a bug is released,
/// then it should be fixed with a compensating Update entry (unless the issue is data destructive
/// obviously!).
///
/// This allows us to code-in schema upgrades, indexes, optimisations etc, without the use of external
/// scripts. Each update is registered in the MongoDB to ensure it is not run multiple times.
///
/// To ensure only one instance of this service runs the updates, a lock entry is made in the database
/// to prevent other instances from also running updates. If an update crashes, this means all nodes are
/// prevented from running the update until action is taken.
///
/// These updates are to be used sparingly and are a means to allow post-releases refactoring of schemas
/// to solve as-yet, unforseen issues (for example putting account, accountmetadata and devices in the same
/// MongoDB document...)
///
/// Recommend any collections used in here are UNTYPED. That way the types in the main code can evolve 
/// without breaking the code in this module.
///

///
/// Only ever append to this list never amend existing database updates that have been released in a build.
///
fn create_update_plan() -> Vec<Update> {
    let mut updates: Vec<Update> = vec!();
    updates.push(Update { id: 1, label: "Create default profiles", apply: Box::new(|db| Box::pin(create_default_profiles(db))) });
    updates.push(Update { id: 2, label: "Create initial indexes",  apply: Box::new(|db| Box::pin(create_init_indexes(db))) });
    updates
}

const UPDATES: &str = "Updates";
const UPDATE_LOCK: &str = "UpdateLock";

type LockId = String;

struct Update {
    label: &'static str,
    id: u32,
    apply: UpdateFn /* async Fn(Data<Database>) -> Result<(), InternalError> */
}

// Full Rust-safe type definitions to hold an async update function in a struct.
type UpdateResult = Result<(), InternalError>;
type AsyncUpdateResult = Pin<Box<dyn Future<Output = UpdateResult>>>;
type UpdateFn = Box<dyn Fn(Arc<Database>) -> AsyncUpdateResult>;

///
/// Run any schema-like updates against MongoDB that haven't been run yet.
///
pub async fn update_mongo(db: &Database, config: &Configuration) -> Result<(), InternalError> {
    let updates = create_update_plan();

    if config.update_schema_enabled {
        // TODO: We can check the version at this point and skip this section if we're up-to-date.

         // TODO: Retry with backoff - other instances (or tests!) may be running.
        let retry_strategy = ExponentialBackoff::from_millis(1000)
            .max_delay(Duration::from_secs(30)) // Make configurable with default.
            .map(jitter); // add jitter to delays
            // .take(3);  // Max retries.

        let lock = Retry::spawn(retry_strategy, || lock_db(&db)).await?;
        // let lock = lock_db(&db).await?;

        if let Some(_lock) = lock {
            let a_db = Arc::new(db.clone()); // We'll need shared references for the update steps.
            for update in &updates {
                if !applied(&update, db).await? {
                    apply(&update, a_db.clone()).await?;
                }
            }

            unlock_db(db).await?
        }
    }

    if !config.allow_out_of_date_schema {
        ensure_db_updated(updates, db).await?;
    }

    Ok(())
}

async fn applied(update: &Update, db: &Database) -> Result<bool, InternalError> {
    let col: Collection<Document> = db.collection(UPDATES);
    let result = col.find_one(doc!{ "id": update.id }, None).await?;
    debug!("Is update {:?} already applied? {}", update.id, result.is_some());
    Ok(result.is_some())
}

async fn apply(update: &Update, db: Arc<Database>) -> Result<(), InternalError> {
    info!("Running MongoDB update {}: {}", update.id, update.label);

    let apply = &update.apply;
    apply(db.clone()).await?;

    // Register the update so it doesn't run again.
    let col = db.collection(UPDATES);
    col.insert_one(doc!{
        "id": update.id,
        "label": update.label,
        "when": Utc::now()
    }, None).await?;

    info!("Completed MongoDB update {}: {}", update.id, update.label);
    Ok(())
}

async fn lock_db(db: &Database) -> Result<Option<LockId>, InternalError> {
    println!("Locking DB tid {:?}", std::thread::current().id());
    // Create a unique index on the lock collection - we'll use this to ensure only one instance can be running updates at once.
    db.run_command(doc! {
        "createIndexes": UPDATE_LOCK,
        "indexes": [
            {
                "key": {
                    "Locked": 1
                },
                "name": "idx_SingleInstanceLock",
                "unique": true
            }
        ]
    }, None).await?;

    // Attempt to insert a record with a hardcoded value for the index.
    let lock_id = Uuid::new_v4().to_hyphenated().to_string();
    let result = db.collection(UPDATE_LOCK)
        .insert_one(doc! { "lockId": lock_id.clone(), "locked": true }, None).await;

    match result {
        Ok(_result) => debug!("Locked the database for updates, lock id {}", lock_id),
        // Err(err) => {
        //     info!("Failed to lock the database for updates, is another instance updating right now? : {}", err.to_string());
        //     return Ok(None)
        // }
        Err(err) => return Err(InternalError::MongoLockedForUpdate { cause: err.to_string() })
    };

    // If successful, we locked the DB from other instances.
    Ok(Some(lock_id))
}

async fn unlock_db(db: &Database) -> Result<(), InternalError> {
    debug!("Unlocking database after update");
    let col: Collection<Document> = db.collection(UPDATE_LOCK);
    col.drop(None).await?;
    Ok(())
}

async fn ensure_db_updated(updates: Vec<Update>, db: &Database) -> Result<(), InternalError> {
    // TODO: retry with a sleep and THEN fail - gives follower instance chance for the leader to update whilst locked.

    // Get the max update id from the database
    let doc: Option<Document> = db.collection(UPDATES)
        .find_one(None, FindOneOptions::builder()
        .projection(doc!{ "id": 1 })
        .sort(doc!{ "id": -1 })
        .build()).await?;

    // Get the update id from the plan.
    let max_plan = updates.iter().max_by_key(|u|u.id).map(|u|u.id as i32).unwrap_or(0);

    let max_db = match &doc {
        Some(doc) => doc.get_i32("id")?,
        None => 0
    };

    if max_plan != max_db {
        return Err(InternalError::MongoSchemaError { code_version: max_plan, db_version: max_db })
    }

    Ok(())
}

async fn create_init_indexes(db: Arc<Database>) -> Result<(), InternalError> {
    // Note: the current driver doesn't yet support creating indexes on collections, so the dbcommand
    // must be used instead.
    // https://docs.mongodb.com/manual/reference/command/createIndexes/#createindexes

    // Note: I've split multiple calls to the same collection to ease readability.
    db.run_command(doc! { "createIndexes": UPDATES, "indexes": [{ "key": { "id": 1 }, "name": "idx_id", "unique": true }] }, None).await?;
    db.run_command(doc! { "createIndexes": "Accounts", "indexes": [{ "key": { "accountId": 1 }, "name": "idx_accountId", "unique": true }] }, None).await?;
    db.run_command(doc! { "createIndexes": "Accounts", "indexes": [{ "key": { "devices.deviceId": 1 }, "name": "idx_deviceId", "unique": true, "sparse": true } ] }, None).await?;
    db.run_command(doc! { "createIndexes": "Accounts", "indexes": [{ "key": { "externalIds.key": 1, "externalIds.value": 1 }, "name": "idx_accountExternalId", "unique": true, "sparse": true }] }, None).await?;
    db.run_command(doc! { "createIndexes": "Accounts", "indexes": [{ "key": { "devices.externalIds.key": 1, "devices.externalIds.value": 1 }, "name": "idx_deviceExternalId", "unique": true, "sparse": true }] }, None).await?;
    db.run_command(doc! { "createIndexes": "AccountProfiles", "indexes": [{ "key": { "profileId": 1 }, "name": "idx_profileId", "unique": true }] }, None).await?;
    db.run_command(doc! { "createIndexes": "DeviceProfiles", "indexes": [{ "key": { "profileId": 1 }, "name": "idx_profileId", "unique": true }] }, None).await?;

    Ok(())
}

async fn create_default_profiles(db: Arc<Database>) -> Result<(), InternalError> {
    let col: Collection = db.collection("AccountProfiles");
    col.insert_one(doc!{ "profileId": "DEFAULT" }, None).await?;

    let col: Collection = db.collection("DeviceProfiles");
    col.insert_one(doc!{ "profileId": "DEFAULT" }, None).await?;
    Ok(())
}
